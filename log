2024-08-09 15:53:48,281 - modelscope - INFO - PyTorch version 2.2.0 Found.
2024-08-09 15:53:48,281 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer
2024-08-09 15:53:48,368 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 fa95f1baa11f4e245779d7d28ea778dd and a total number of 980 components indexed
XGPU-lite: L-138:vGPUs:VGPU-d04177ba-20e5-022d-04ec-f0d0067fe3ad.0 count:1 pGPUs:GPU-d04177ba-20e5-022d-04ec-f0d0067fe3ad
XGPU-lite: L-61:vgpu client init done, hostID:bde05811582b PID:150079 gpuCnt:1 vir-ratio:1
XGPU-lite: L-86:pGPU used:GPU-d04177ba-20e5-022d-04ec-f0d0067fe3ad
XGPU-lite: L-98:Job hostID:bde05811582b pid:150079 desc:python3.8 connecting to XGPU service ...
XGPU-lite: L-127:HostID:bde05811582b pid:150079 XGPU connected jobIdx:0 token:0
XGPU-lite: L-146:Client configuration: use_uma = 1, compute_schedule_mode: 4, need_launch_kernel_admission: 0, time_slice_spin_or_cv: 1, enable_heart_beat: 0, enable_monitor: 0.
XGPU-lite: L-163:func: cuInit, pid: 150079, tid: 150079, flags: 0
I0809 15:53:51.134299: nvml_client.cc:103] Native NVML Libray has already been initialized, skip library handle init

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda121.so
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 121
CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...
[2024-08-09 15:53:51,134] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py:741: FutureWarning: ignore(True) has been deprecated. TorchScript will now drop the function call on compilation. Use torch.jit.unused now. {}
  warnings.warn(
/opt/conda/lib/python3.8/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.
  deprecate("LoRACompatibleLinear", "1.0.0", deprecation_message)
2024-08-09 15:54:09,342 INFO input frame rate=50
/opt/conda/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")
/ossfs/workspace/CosyVoice/cosyvoice/dataset/processor.py:24: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend('soundfile')
/opt/conda/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'
  warnings.warn(
2024-08-09 15:54:19,350 WETEXT INFO found existing fst: /opt/conda/lib/python3.8/site-packages/tn/zh_tn_tagger.fst
2024-08-09 15:54:19,350 INFO found existing fst: /opt/conda/lib/python3.8/site-packages/tn/zh_tn_tagger.fst
2024-08-09 15:54:19,350 WETEXT INFO                     /opt/conda/lib/python3.8/site-packages/tn/zh_tn_verbalizer.fst
2024-08-09 15:54:19,350 INFO                     /opt/conda/lib/python3.8/site-packages/tn/zh_tn_verbalizer.fst
2024-08-09 15:54:19,350 WETEXT INFO skip building fst for zh_normalizer ...
2024-08-09 15:54:19,350 INFO skip building fst for zh_normalizer ...
2024-08-09 15:54:19,623 WETEXT INFO found existing fst: /opt/conda/lib/python3.8/site-packages/tn/en_tn_tagger.fst
2024-08-09 15:54:19,623 INFO found existing fst: /opt/conda/lib/python3.8/site-packages/tn/en_tn_tagger.fst
2024-08-09 15:54:19,623 WETEXT INFO                     /opt/conda/lib/python3.8/site-packages/tn/en_tn_verbalizer.fst
2024-08-09 15:54:19,623 INFO                     /opt/conda/lib/python3.8/site-packages/tn/en_tn_verbalizer.fst
2024-08-09 15:54:19,623 WETEXT INFO skip building fst for en_normalizer ...
2024-08-09 15:54:19,623 INFO skip building fst for en_normalizer ...
XGPU-lite: L-163:func: cuInit, pid: 150079, tid: 150079, flags: 0
XGPU-lite: L-163:func: cuInit, pid: 150079, tid: 150079, flags: 0
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
failed to import ttsfrd, use WeTextProcessing instead
xs.shape, att_mask.shape, pos_emb.shape torch.Size([1, 253, 1024]) torch.Size([1, 253, 253]) torch.Size([1, 505, 1024])
Traceback (most recent call last):
  File "test.py", line 9, in <module>
    for idx, itr in enumerate(output):
  File "/ossfs/workspace/CosyVoice/cosyvoice/cli/cosyvoice.py", line 63, in inference_zero_shot
    for model_output in self.model.inference(**model_input, stream=stream):
  File "/ossfs/workspace/CosyVoice/cosyvoice/cli/model.py", line 131, in inference
    for i in self.llm.inference(text=text.to(self.device),
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/ossfs/workspace/CosyVoice/cosyvoice/llm/llm.py", line 205, in inference
    y_pred, att_cache, cnn_cache = self.llm.forward_chunk(
  File "/ossfs/workspace/CosyVoice/cosyvoice/transformer/encoder.py", line 248, in forward_chunk
    print("xs.shape, att_mask.shape, pos_emb.shape", xs.shape, att_mask.shape, pos_emb.shape)
AttributeError: 'NoneType' object has no attribute 'shape'
2024-08-09 15:54:29,276 DEBUG Attempting to acquire lock 140510188214304 on /root/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
2024-08-09 15:54:29,277 DEBUG Lock 140510188214304 acquired on /root/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
2024-08-09 15:54:29,277 DEBUG Attempting to release lock 140510188214304 on /root/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
2024-08-09 15:54:29,277 DEBUG Lock 140510188214304 released on /root/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
2024-08-09 15:54:29,745 DEBUG Attempting to acquire lock 140510195582048 on /root/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
2024-08-09 15:54:29,745 DEBUG Lock 140510195582048 acquired on /root/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
2024-08-09 15:54:29,746 DEBUG Attempting to release lock 140510195582048 on /root/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
2024-08-09 15:54:29,746 DEBUG Lock 140510195582048 released on /root/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
XGPU-lite: L-256:JobId:0 pid:150079 disconncect XGPU service ...
XGPU-lite: L-79:-------------------------
XGPU-lite: L-80:Destroy xgpu_client_t
XGPU-lite: L-81:-------------------------
